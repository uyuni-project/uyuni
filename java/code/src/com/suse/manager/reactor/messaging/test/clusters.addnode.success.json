{
  "tag": "salt/job/20190218111637161612/ret/${minion-id}",
  "data": {
    "_stamp": "2020-05-18T15:50:46.891142",
    "cmd": "_return",
    "fun": "state.apply",
    "fun_args": [
      {
        "mods": [
          "clusters.addnode"
        ],
        "pillar": {
          "cluster_type": "caasp",
          "params": {
            "node_name": "dev-min-caasp-worker-1",
            "role": "worker",
            "skuba_cluster_path": "/opt/clusters/mycluster",
            "ssh_key_file": "/root/.ssh/id_rsa",
            "target": "dev-min-caasp-worker-1.lan",
            "user": "root"
          },
          "state_hooks": {
            "join": {
              "after": "caasp.kill_ssh_agent",
              "before": "caasp.init_ssh_agent"
            },
            "remove": {
              "after": "caasp.kill_ssh_agent",
              "before": "caasp.init_ssh_agent"
            },
            "upgrade": {
              "after": "caasp.kill_ssh_agent",
              "before": "caasp.init_ssh_agent"
            }
          }
        },
        "queue": true
      }
    ],
    "id": "dev-min-sles15sp1.lan",
    "jid": "20200518154834653359",
    "metadata": {
      "batch-mode": true,
      "suma-action-chain": false,
      "suma-action-id": ${action1-id},
      "suma-force-pkg-list-refresh": false,
      "suma-minion-startup": false
    },
    "out": "highstate",
    "retcode": 0,
    "return": {
      "module_|-mgr_caasp_add_key_|-ssh_agent.add_key_|-run": {
        "__id__": "mgr_caasp_add_key",
        "__run_num__": 2,
        "__sls__": "caasp.init_ssh_agent",
        "changes": {
          "ret": true
        },
        "comment": "Module function ssh_agent.add_key executed",
        "duration": 23.659,
        "name": "ssh_agent.add_key",
        "result": true,
        "start_time": "17:48:35.303112"
      },
      "module_|-mgr_caasp_kill_agent_|-ssh_agent.kill_|-run": {
        "__id__": "mgr_caasp_kill_agent",
        "__run_num__": 5,
        "__sls__": "caasp.kill_ssh_agent",
        "changes": {
          "ret": true
        },
        "comment": "Module function ssh_agent.kill executed",
        "duration": 24.596,
        "name": "ssh_agent.kill",
        "result": true,
        "start_time": "17:50:46.767061"
      },
      "module_|-mgr_caasp_list_keys_|-ssh_agent.list_keys_|-run": {
        "__id__": "mgr_caasp_list_keys",
        "__run_num__": 3,
        "__sls__": "caasp.init_ssh_agent",
        "changes": {
          "ret": "2048 SHA256:bXj0hkqpDm2dN2gaKHSC64KqLCJ7TiGY4j1whuI6kl4 root@dev-min-sles15sp1 (RSA)"
        },
        "comment": "Module function ssh_agent.list_keys executed",
        "duration": 22.523,
        "name": "ssh_agent.list_keys",
        "result": true,
        "start_time": "17:48:35.327653"
      },
      "module_|-mgr_caasp_load_ssh_agent_|-ssh_agent.start_agent_|-run": {
        "__id__": "mgr_caasp_load_ssh_agent",
        "__run_num__": 1,
        "__sls__": "caasp.init_ssh_agent",
        "changes": {
          "ret": {
            "SSH_AGENT_PID": "13826",
            "SSH_AUTH_SOCK": "/tmp/ssh-4SC511kt1rSU/agent.13825"
          }
        },
        "comment": "Module function ssh_agent.start_agent executed",
        "duration": 38.515,
        "name": "ssh_agent.start_agent",
        "result": true,
        "start_time": "17:48:35.263595"
      },
      "module_|-mgr_cluster_add_node_dev-min-caasp-worker-1.lan_|-mgrclusters.add_node_|-run": {
        "__id__": "mgr_cluster_add_node",
        "__run_num__": 4,
        "__sls__": "clusters.addnode",
        "changes": {
          "ret": {
            "retcode": 0,
            "stderr": "W0518 17:48:35.854168   13844 ssh.go:311] \nThe authenticity of host '192.168.1.208:22' can't be established.\nECDSA key fingerprint is 50:08:8f:ba:b1:75:68:4c:0a:3b:f0:6b:0b:4f:4f:0c.\nI0518 17:48:35.854333   13844 ssh.go:312] accepting SSH key for \"dev-min-caasp-worker-1.lan:22\"\nI0518 17:48:35.854365   13844 ssh.go:313] adding fingerprint for \"dev-min-caasp-worker-1.lan:22\" to \"known_hosts\"\nE0518 17:50:14.789115   13844 ssh.go:195] W0518 17:50:09.919982   22114 removeetcdmember.go:79] [reset] No kubeadm config, using etcd pod spec to get data directory\nE0518 17:50:16.834438   13844 ssh.go:195] W0518 17:50:11.965032   22114 cleanupnode.go:81] [reset] Failed to remove containers: output: time=\"2020-05-18T17:50:11+02:00\" level=fatal msg=\"failed to connect: failed to connect, make sure you are running as root and the runtime has been started: context deadline exceeded\"\nE0518 17:50:16.834464   13844 ssh.go:195] , error: exit status 1\nE0518 17:50:17.265201   13844 ssh.go:195] No files found for firewalld.service.\nE0518 17:50:20.986180   13844 ssh.go:195] Created symlink /etc/systemd/system/multi-user.target.wants/crio.service → /usr/lib/systemd/system/crio.service.\nE0518 17:50:24.071198   13844 ssh.go:195] Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service → /usr/lib/systemd/system/kubelet.service.\nE0518 17:50:46.509489   13844 ssh.go:195] Created symlink /etc/systemd/system/timers.target.wants/skuba-update.timer → /usr/lib/systemd/system/skuba-update.timer.\n",
            "stdout": "[join] applying states to new node\n[join] node successfully joined the cluster\n",
            "success": true
          }
        },
        "comment": "Module function mgrclusters.add_node executed",
        "duration": 131414.254,
        "name": "mgrclusters.add_node",
        "result": true,
        "start_time": "17:48:35.351066"
      },
      "module_|-mgr_cluster_add_node_dev-min-caasp-worker-2.lan_|-mgrclusters.add_node_|-run": {
        "__id__": "mgr_cluster_add_node",
        "__run_num__": 4,
        "__sls__": "clusters.addnode",
        "changes": {
          "ret": {
            "retcode": 0,
            "stderr": "W0518 17:48:35.854168   13844 ssh.go:311] \nThe authenticity of host '192.168.1.208:22' can't be established.\nECDSA key fingerprint is 50:08:8f:ba:b1:75:68:4c:0a:3b:f0:6b:0b:4f:4f:0c.\nI0518 17:48:35.854333   13844 ssh.go:312] accepting SSH key for \"dev-min-caasp-worker-2.lan:22\"\nI0518 17:48:35.854365   13844 ssh.go:313] adding fingerprint for \"dev-min-caasp-worker-2.lan:22\" to \"known_hosts\"\nE0518 17:50:14.789115   13844 ssh.go:195] W0518 17:50:09.919982   22114 removeetcdmember.go:79] [reset] No kubeadm config, using etcd pod spec to get data directory\nE0518 17:50:16.834438   13844 ssh.go:195] W0518 17:50:11.965032   22114 cleanupnode.go:81] [reset] Failed to remove containers: output: time=\"2020-05-18T17:50:11+02:00\" level=fatal msg=\"failed to connect: failed to connect, make sure you are running as root and the runtime has been started: context deadline exceeded\"\nE0518 17:50:16.834464   13844 ssh.go:195] , error: exit status 1\nE0518 17:50:17.265201   13844 ssh.go:195] No files found for firewalld.service.\nE0518 17:50:20.986180   13844 ssh.go:195] Created symlink /etc/systemd/system/multi-user.target.wants/crio.service → /usr/lib/systemd/system/crio.service.\nE0518 17:50:24.071198   13844 ssh.go:195] Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service → /usr/lib/systemd/system/kubelet.service.\nE0518 17:50:46.509489   13844 ssh.go:195] Created symlink /etc/systemd/system/timers.target.wants/skuba-update.timer → /usr/lib/systemd/system/skuba-update.timer.\n",
            "stdout": "[join] applying states to new node\n[join] node successfully joined the cluster\n",
            "success": true
          }
        },
        "comment": "Module function mgrclusters.add_node executed",
        "duration": 131414.254,
        "name": "mgrclusters.add_node",
        "result": true,
        "start_time": "17:48:35.351066"
      },
      "module_|-sync_modules_|-saltutil.sync_modules_|-run": {
        "__id__": "sync_modules",
        "__run_num__": 0,
        "__sls__": "util.syncmodules",
        "changes": {
          "ret": []
        },
        "comment": "Module function saltutil.sync_modules executed",
        "duration": 204.46,
        "name": "saltutil.sync_modules",
        "result": true,
        "start_time": "17:48:35.058905"
      }
    },
    "success": true
  }
}